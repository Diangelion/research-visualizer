{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNxkH7QSwJgNCNhBGxj65Z4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Diangelion/research-visualizer/blob/main/research_visualizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-community langchain-huggingface PyPDF2"
      ],
      "metadata": {
        "id": "RlCQ4yptuZ_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image Generator"
      ],
      "metadata": {
        "id": "WOVOmYnl3o51"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import os\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "from google.colab import files\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from diffusers import StableDiffusion3Pipeline\n",
        "from PyPDF2 import PdfReader\n",
        "from transformers import (\n",
        "    pipeline,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoModelForSeq2SeqLM\n",
        ")\n",
        "from diffusers import StableDiffusionPipeline"
      ],
      "metadata": {
        "id": "ng2nEHbi40a4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#=======================\n",
        "# Configuration\n",
        "#=======================\n",
        "CONFIG = {\n",
        "    \"RESET_DB\": False,\n",
        "    \"text_gen\": {\n",
        "        \"task\": \"text-generation\",\n",
        "        \"repo_id\": \"Qwen/Qwen3-1.7B\"\n",
        "    },\n",
        "    \"summarize\": {\n",
        "        \"task\": \"summarization\",\n",
        "        \"repo_id\": \"nsi319/legal-led-base-16384\"\n",
        "    },\n",
        "    \"image_gen\": {\n",
        "        \"task\": \"image-to-text\",\n",
        "        \"repo_id\": \"sd-legacy/stable-diffusion-v1-5\"\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "p2qlMQZv4gwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pdf():\n",
        "  uploaded = files.upload()\n",
        "\n",
        "  if len(uploaded) != 1:\n",
        "    raise ValueError(\"Please upload exactly ONE PDF file\")\n",
        "\n",
        "  filename, content = next(iter(uploaded.items()))\n",
        "\n",
        "  if not filename.lower().endswith('.pdf'):\n",
        "    raise ValueError(\"Only PDF files are accepted\")\n",
        "\n",
        "  # Read PDF directly from bytes\n",
        "  pdf_reader = PdfReader(io.BytesIO(content))\n",
        "  file_text = \"\\n\\n\".join([page.extract_text() for page in pdf_reader.pages])\n",
        "\n",
        "  return file_text"
      ],
      "metadata": {
        "id": "yykAVR8C5iA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_summarizer(paper_text):\n",
        "  tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"summarize\"][\"repo_id\"])\n",
        "  model = AutoModelForSeq2SeqLM.from_pretrained(CONFIG[\"summarize\"][\"repo_id\"])\n",
        "  padding = \"max_length\"\n",
        "  input_tokenized = tokenizer.encode(\n",
        "      paper_text,\n",
        "      return_tensors='pt',\n",
        "      padding=padding,\n",
        "      pad_to_max_length=True,\n",
        "      max_length=16384,\n",
        "      truncation=True\n",
        "  )\n",
        "  summary_ids = model.generate(\n",
        "      input_tokenized,\n",
        "      num_beams=4,\n",
        "      no_repeat_ngram_size=3,\n",
        "      length_penalty=2,\n",
        "      min_length=500,\n",
        "      max_length=1000\n",
        "  )\n",
        "  summary = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids][0]\n",
        "  return { \"summary\": summary }"
      ],
      "metadata": {
        "id": "QwSfWyViBRmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_visualizer():\n",
        "  tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"text_gen\"][\"repo_id\"])\n",
        "  model = AutoModelForCausalLM.from_pretrained(\n",
        "      CONFIG[\"text_gen\"][\"repo_id\"],\n",
        "      torch_dtype=\"auto\",\n",
        "      device_map=\"auto\"\n",
        "  )\n",
        "  pipe = pipeline(\n",
        "    task=CONFIG[\"text_gen\"][\"task\"],\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=32768\n",
        "  )\n",
        "  visualizer_model = HuggingFacePipeline(pipeline=pipe)\n",
        "  template = (\n",
        "      \"\"\"\n",
        "      You are a scientific research visualizator expert.\n",
        "      Create a DETAILED description for an illustration to accompany the summary of a research paper below.\n",
        "\n",
        "      SUMMARY\n",
        "      {summary}\n",
        "      \"\"\"\n",
        "  )\n",
        "  visualizer_prompt = PromptTemplate.from_template(template)\n",
        "  return visualizer_model, visualizer_prompt"
      ],
      "metadata": {
        "id": "FWubjKPc4FVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_image(image_prompt_text):\n",
        "  pipe = StableDiffusionPipeline.from_pretrained(\n",
        "      CONFIG[\"image_gen\"][\"repo_id\"],\n",
        "      torch_dtype=torch.float16\n",
        "  )\n",
        "  pipe = pipe.to(\"cuda\")\n",
        "  image = pipe(image_prompt_text).images[0]\n",
        "\n",
        "  plt.imshow(image)\n",
        "  plt.axis('off')\n",
        "  plt.show()\n",
        "\n",
        "  return { \"ai_prompt\": image_prompt_text}"
      ],
      "metadata": {
        "id": "Jhw0AI8G4xyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_chain(\n",
        "    visualizer_model: HuggingFacePipeline,\n",
        "    visualizer_prompt: PromptTemplate\n",
        "):\n",
        "  chain = (\n",
        "    (lambda x: create_summarizer(x[\"text_paper\"]))\n",
        "    | visualizer_prompt\n",
        "    | visualizer_model\n",
        "    | StrOutputParser()\n",
        "    | generate_image\n",
        "  )\n",
        "\n",
        "  return chain"
      ],
      "metadata": {
        "id": "1pc_eAJsMs2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "  try:\n",
        "    file_text = get_pdf()\n",
        "    visualizer_model, visualizer_prompt = create_visualizer()\n",
        "    chain = get_chain(visualizer_model, visualizer_prompt)\n",
        "    response = chain.invoke({ \"text_paper\": file_text })\n",
        "    print(f\"Response: {response}\")\n",
        "  except Exception as e:\n",
        "    print(f\"Error: {str(e)}\")"
      ],
      "metadata": {
        "id": "o6g4Fmcq6gxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "metadata": {
        "id": "3GZHY69awc7y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}